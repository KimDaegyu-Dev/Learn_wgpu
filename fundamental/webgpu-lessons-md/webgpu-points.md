English Spanish Êó•Êú¨Ë™û ÌïúÍµ≠Ïñ¥ –†—É—Å—Å–∫–∏–π T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÁÆÄ‰Ωì‰∏≠Êñá [Table of Contents](#toc) 

# [webgpufundamentals.org](/)

#forkongithub>div { background: #000; color: #fff; font-family: arial,sans-serif; text-align: center; font-weight: bold; padding: 5px 40px; font-size: 0.9rem; line-height: 1.3rem; position: relative; transition: 0.5s; display: block; width: 400px; position: absolute; top: 0; right: 0; transform: translateX(160px) rotate(45deg) translate(10px,70px); box-shadow: 4px 4px 10px rgba(0,0,0,0.8); pointer-events: auto; } #forkongithub a { text-decoration: none; color: #fff; } #forkongithub>div:hover { background: #c11; color: #fff; } #forkongithub .contributors { font-size: 0.75rem; background: rgba(255,255,255,0.2); line-height: 1.2; padding: 0.1em; } #forkongithub>div::before,#forkongithub>div::after { content: ""; width: 100%; display: block; position: absolute; top: 1px; left: 0; height: 1px; background: #fff; } #forkongithub>div::after { bottom: 1px; top: auto; } #forkongithub{ z-index: 9999; /\* needed for firefox \*/ overflow: hidden; width: 300px; height: 300px; position: absolute; right: 0; top: 0; pointer-events: none; } #forkongithub svg{ width: 1em; height: 1em; vertical-align: middle; } #forkongithub img { width: 1em; height: 1em; border-radius: 100%; vertical-align: middle; } @media (max-width: 900px) { #forkongithub>div { line-height: 1.2rem; } } @media (max-width: 700px) { #forkongithub { display: none; } } @media (max-width: 410px) { #forkongithub>div { font-size: 0.7rem; transform: translateX(150px) rotate(45deg) translate(20px,40px); } }

[Fix, Fork, Contribute](https://github.com/webgpu/webgpufundamentals)

# WebGPU Points

WebGPU supports drawing to points. We do this by setting the primitive topology to `'point-list'` in a render pipeline.

Let‚Äôs create a simple example with random points starting with ideas presented in [the article on vertex buffers](webgpu-vertex-buffers.html).

First, a simple vertex shader and fragment shader. To keep it simple we‚Äôll just use clip space coordinates for positions and hard code the color yellow in our fragment shader.

struct Vertex {
  @location(0) position: vec2f,
};

struct VSOutput {
  @builtin(position) position: vec4f,
};

@vertex fn vs(vert: Vertex,) -> VSOutput {
  var vsOut: VSOutput;
  vsOut.position = vert.position;
  return vsOut;
}

@fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
  return vec4f(1, 1, 0, 1); // yellow
}

Then, when we create a pipeline, we set the topology to `'point-list'`

  const pipeline = device.createRenderPipeline({
    label: '1 pixel points',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
          arrayStride: 2 \* 4, // 2 floats, 4 bytes each
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x2'},  // position
          \],
        },
      \],
    },
    fragment: {
      module,
      targets: \[{ format: presentationFormat }\],
    },
+    primitive: {
+      topology: 'point-list',
+    },
  });

Let‚Äôs fill a vertex buffer with some random clips space points

  const rand = (min, max) => min + Math.random() \* (max - min);

  const kNumPoints = 100;
  const vertexData = new Float32Array(kNumPoints \* 2);
  for (let i = 0; i < kNumPoints; ++i) {
    const offset = i \* 2;
    vertexData\[offset + 0\] = rand(-1, 1);
    vertexData\[offset + 1\] = rand(-1, 1);
  }

  const vertexBuffer = device.createBuffer({
    label: 'vertex buffer vertices',
    size: vertexData.byteLength,
    usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY\_DST,
  });
  device.queue.writeBuffer(vertexBuffer, 0, vertexData);

And then draw

    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
    pass.setVertexBuffer(0, vertexBuffer);
    pass.draw(kNumPoints);
    pass.end();

And with that we get 100 random yellow points

[click here to open in a separate window](/webgpu/lessons/../webgpu-points.html)

Unfortunately they are all only 1 pixel in size. 1 pixel size points is all WebGPU supports. If we want something larger we need to do it ourselves. Fortunately it‚Äôs easy to do. We‚Äôll just make a quad and use [instancing](webgpu-vertex-buffers.html#a-instancing);

Let‚Äôs add a quad to our vertex shader and a size attribute. Let‚Äôs also add a uniform to pass in the size of the texture we are drawing to.

struct Vertex {
  @location(0) position: vec2f,
+  @location(1) size: f32,
};

+struct Uniforms {
+  resolution: vec2f,
+};

struct VSOutput {
  @builtin(position) position: vec4f,
};

+@group(0) @binding(0) var<uniform> uni: Uniforms;

@vertex fn vs(
    vert: Vertex,
+    @builtin(vertex\_index) vNdx: u32,
) -> VSOutput {
+  let points = array(
+    vec2f(-1, -1),
+    vec2f( 1, -1),
+    vec2f(-1,  1),
+    vec2f(-1,  1),
+    vec2f( 1, -1),
+    vec2f( 1,  1),
+  );
  var vsOut: VSOutput;
+  let pos = points\[vNdx\];
-  vsOut.position = vec4f(vert.position, 0, 1);
+  vsOut.position = vec4f(vert.position + pos \* vert.size / uni.resolution, 0, 1);
  return vsOut;
}

@fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
  return vec4f(1, 1, 0, 1); // yellow
}

In JavaScript we need to add an attribute for a size per point, we need to set the attributes to advance per instance by setting `stepMode: 'instance'`, and we can remove the topology setting since we want the default `'triangle-list'`

  const pipeline = device.createRenderPipeline({
    label: 'sizeable points',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
-          arrayStride: 2 \* 4, // 2 floats, 4 bytes each
+          arrayStride: (2 + 1) \* 4, // 3 floats, 4 bytes each
+          stepMode: 'instance',
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x2'},  // position
+            {shaderLocation: 1, offset: 8, format: 'float32'},  // size
          \],
        },
      \],
    },
    fragment: {
      module,
      targets: \[{ format: presentationFormat }\],
    },
-    primitive: {
-      topology: 'point-list',
-    },
  });

Let‚Äôs add a random size per point to our vertex data

  const kNumPoints = 100;
-  const vertexData = new Float32Array(kNumPoints \* 2);
+  const vertexData = new Float32Array(kNumPoints \* 3);
  for (let i = 0; i < kNumPoints; ++i) {
-    const offset = i \* 2;
+    const offset = i \* 3;
    vertexData\[offset + 0\] = rand(-1, 1);
    vertexData\[offset + 1\] = rand(-1, 1);
+    vertexData\[offset + 2\] = rand(1, 32);
  }

We need a uniform buffer so we can pass in the resolution

  const uniformValues = new Float32Array(2);
  const uniformBuffer = device.createBuffer({
    size: uniformValues.byteLength,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY\_DST,
  });
  const kResolutionOffset = 0;
  const resolutionValue = uniformValues.subarray(
      kResolutionOffset, kResolutionOffset + 2);

And we need a bind group to bind the uniform buffer

  const bindGroup = device.createBindGroup({
    layout: pipeline.getBindGroupLayout(0),
    entries: \[
      { binding: 0, resource: { buffer: uniformBuffer }},
    \],
  });

Then at render time we can update the uniform buffer with the current resolution.

    // Get the current texture from the canvas context and
    // set it as the texture to render to.
    const canvasTexture = context.getCurrentTexture();
    renderPassDescriptor.colorAttachments\[0\].view =
        canvasTexture.createView();

+    // Update the resolution in the uniform buffer
+    resolutionValue.set(\[canvasTexture.width, canvasTexture.height\]);
+    device.queue.writeBuffer(uniformBuffer, 0, uniformValues);

then set our bind group and render an instance per point

    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
    pass.setVertexBuffer(0, vertexBuffer);
+    pass.setBindGroup(0, bindGroup);
-    pass.draw(kNumPoints);
+    pass.draw(6, kNumPoints);
    pass.end();

And now we have sizable points

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-w-size.html)

What if we wanted to texture our points? We just need to pass in texture coordinates from the vertex shader to the fragment shader.

struct Vertex {
  @location(0) position: vec2f,
  @location(1) size: f32,
};

struct Uniforms {
  resolution: vec2f,
};

struct VSOutput {
  @builtin(position) position: vec4f,
+  @location(0) texcoord: vec2f,
};

@group(0) @binding(0) var<uniform> uni: Uniforms;

@vertex fn vs(
    vert: Vertex,
    @builtin(vertex\_index) vNdx: u32,
) -> VSOutput {
  let points = array(
    vec2f(-1, -1),
    vec2f( 1, -1),
    vec2f(-1,  1),
    vec2f(-1,  1),
    vec2f( 1, -1),
    vec2f( 1,  1),
  );
  var vsOut: VSOutput;
  let pos = points\[vNdx\];
  vsOut.position = vec4f(vert.position + pos \* vert.size / uni.resolution, 0, 1);
+  vsOut.texcoord = pos \* 0.5 + 0.5;
  return vsOut;
}

And of course use a texture in the fragment shader

+@group(0) @binding(1) var s: sampler;
+@group(0) @binding(2) var t: texture\_2d<f32>;

@fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
-  return vec4f(1, 1, 0, 1); // yellow
+  return textureSample(t, s, vsOut.texcoord);
}

We‚Äôll create a simple texture using a canvas like we covered in [the article on importing textures](webgpu-importing-textures.html).

  const ctx = new OffscreenCanvas(32, 32).getContext('2d');
  ctx.font = '27px sans-serif';
  ctx.textAlign = 'center';
  ctx.textBaseline = 'middle';
  ctx.fillText('ü•ë', 16, 16);

  const texture = device.createTexture({
    size: \[32, 32\],
    format: 'rgba8unorm',
    usage: GPUTextureUsage.TEXTURE\_BINDING |
           GPUTextureUsage.COPY\_DST |
           GPUTextureUsage.RENDER\_ATTACHMENT,
  });
  device.queue.copyExternalImageToTexture(
    { source: ctx.canvas, flipY: true },
    { texture, premultipliedAlpha: true },
    \[32, 32\],
  );

And we need a sampler and we need to add them to our bind group

  const sampler = device.createSampler({
    minFilter: 'linear',
    magFilter: 'linear',
  });

  const bindGroup = device.createBindGroup({
    layout: pipeline.getBindGroupLayout(0),
    entries: \[
      { binding: 0, resource: { buffer: uniformBuffer }},
+      { binding: 1, resource: sampler },
+      { binding: 2, resource: texture.createView() },
    \],
  });

Let‚Äôs also turn on blending so we get [transparency](webgpu-transparency.html)

  const pipeline = device.createRenderPipeline({
    label: 'sizeable points with texture',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
          arrayStride: (2 + 1) \* 4, // 3 floats, 4 bytes each
          stepMode: 'instance',
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x2'},  // position
            {shaderLocation: 1, offset: 8, format: 'float32'},  // size
          \],
        },
      \],
    },
    fragment: {
      module,
-      targets: \[{ format: presentationFormat }\],
+      targets: \[
+        {
+         format: presentationFormat,
+          blend: {
+            color: {
+              srcFactor: 'one',
+              dstFactor: 'one-minus-src-alpha',
+              operation: 'add',
+            },
+            alpha: {
+              srcFactor: 'one',
+              dstFactor: 'one-minus-src-alpha',
+              operation: 'add',
+            },
+          },
+        },
+      \],
    },
  });

And now we have textured points

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-w-texture.html)

And we could keep going, how about a rotation per point? Using the math we covered in [the article on matrix math](webgpu-matrix-math.html).

struct Vertex {
  @location(0) position: vec2f,
  @location(1) size: f32,
+  @location(2) rotation: f32,
};

struct Uniforms {
  resolution: vec2f,
};

struct VSOutput {
  @builtin(position) position: vec4f,
  @location(0) texcoord: vec2f,
};

@group(0) @binding(0) var<uniform> uni: Uniforms;

@vertex fn vs(
    vert: Vertex,
    @builtin(vertex\_index) vNdx: u32,
) -> VSOutput {
  let points = array(
    vec2f(-1, -1),
    vec2f( 1, -1),
    vec2f(-1,  1),
    vec2f(-1,  1),
    vec2f( 1, -1),
    vec2f( 1,  1),
  );
  var vsOut: VSOutput;
  let pos = points\[vNdx\];
+  let c = cos(vert.rotation);
+  let s = sin(vert.rotation);
+  let rot = mat2x2f(
+     c, s,
+    -s, c,
+  );
-  vsOut.position = vec4f(vert.position + pos \* vert.size / uni.resolution, 0, 1);
+  vsOut.position = vec4f(vert.position + rot \* pos \* vert.size / uni.resolution, 0, 1);
  vsOut.texcoord = pos \* 0.5 + 0.5;
  return vsOut;
      }

We need to add the rotation attribute to our pipeline

  const pipeline = device.createRenderPipeline({
    label: 'sizeable rotatable points with texture',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
-          arrayStride: (2 + 1) \* 4, // 3 floats, 4 bytes each
+          arrayStride: (2 + 1 + 1) \* 4, // 4 floats, 4 bytes each
          stepMode: 'instance',
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x2'},  // position
            {shaderLocation: 1, offset: 8, format: 'float32'},  // size
+            {shaderLocation: 2, offset: 12, format: 'float32'},  // rotation
          \],
        },
      \],
    },
    ...

We need to add rotation to our vertex data

  const kNumPoints = 100;
-  const vertexData = new Float32Array(kNumPoints \* 3);
+  const vertexData = new Float32Array(kNumPoints \* 4);
  for (let i = 0; i < kNumPoints; ++i) {
-    const offset = i \* 3;
+    const offset = i \* 4;
    vertexData\[offset + 0\] = rand(-1, 1);
    vertexData\[offset + 1\] = rand(-1, 1);
\*    vertexData\[offset + 2\] = rand(10, 64);
+    vertexData\[offset + 3\] = rand(0, Math.PI \* 2);
  }

Let‚Äôs also change the texture from ü•ë to üëâ

\-  ctx.fillText('ü•ë', 16, 16);
+  ctx.fillText('üëâ', 16, 16);

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-w-rotation.html)

# What about points in 3D?

The simple answer is just add in the quad values after doing [the 3d math for the vertices](webgpu-perspective-projection.html).

For example, here‚Äôs some code to make 3d positions for a [fibonacci sphere](https://www.google.com/search?q=fibonacci+sphere).

function createFibonacciSphereVertices({
  numSamples,
  radius,
}) {
  const vertices = \[\];
  const increment = Math.PI \* (3 - Math.sqrt(5));
  for (let i = 0; i < numSamples; ++i) {
    const offset = 2 / numSamples;
    const y = ((i \* offset) - 1) + (offset / 2);
    const r = Math.sqrt(1 - Math.pow(y, 2));
    const phi = (i % numSamples) \* increment;
    const x = Math.cos(phi) \* r;
    const z = Math.sin(phi) \* r;
    vertices.push(x \* radius, y \* radius, z \* radius);
  }
  return new Float32Array(vertices);
}

We can draw the vertices with points by applying 3D math to the vertices like [we covered in the series on 3d math](webgpu-cameras.js).

struct Vertex {
  @location(0) position: vec4f,
};

struct Uniforms {
\*  matrix: mat4x4f,
};

struct VSOutput {
  @builtin(position) position: vec4f,
};

@group(0) @binding(0) var<uniform> uni: Uniforms;

@vertex fn vs(
    vert: Vertex,
) -> VSOutput {
  var vsOut: VSOutput;
\*  let clipPos = uni.matrix \* vert.position;
  vsOut.position = clipPos;
  return vsOut;
}

@fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
  return vec4f(1, 0.5, 0.2, 1);  // orange
}

Here‚Äôs our pipeline and vertex buffer

  const pipeline = device.createRenderPipeline({
    label: '3d points with fixed size',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
          arrayStride: (3) \* 4, // 3 floats, 4 bytes each
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x3'},  // position
          \],
        },
      \],
    },
    fragment: {
      module,
      targets: \[
        {
         format: presentationFormat,
        },
      \],
    },
    primitive: {
      topology: 'point-list',
    },
  });

  const vertexData = createFibonacciSphereVertices({
    radius: 1,
    numSamples: 1000,
  });
  const kNumPoints = vertexData.length / 3;

  const vertexBuffer = device.createBuffer({
    label: 'vertex buffer vertices',
    size: vertexData.byteLength,
    usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY\_DST,
  });
  device.queue.writeBuffer(vertexBuffer, 0, vertexData);

And, a uniform buffer and uniform values for our matrix as well as a bindGroup to pass the uniform buffer our shader.

  const uniformValues = new Float32Array(16);
  const uniformBuffer = device.createBuffer({
    size: uniformValues.byteLength,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY\_DST,
  });
  const kMatrixOffset = 0;
  const matrixValue = uniformValues.subarray(
      kMatrixOffset, kMatrixOffset + 16);

  const bindGroup = device.createBindGroup({
    layout: pipeline.getBindGroupLayout(0),
    entries: \[
      { binding: 0, resource: { buffer: uniformBuffer }},
    \],
  });

And the code to draw using a projection matrix, camera, and other 3d math.

  function render(time) {
    time \*= 0.001;

    // Get the current texture from the canvas context and
    // set it as the texture to render to.
    const canvasTexture = context.getCurrentTexture();
    renderPassDescriptor.colorAttachments\[0\].view =
        canvasTexture.createView();

    // Set the matrix in the uniform buffer
    const fov = 90 \* Math.PI / 180;
    const aspect = canvas.clientWidth / canvas.clientHeight;
    const projection = mat4.perspective(fov, aspect, 0.1, 50);
    const view = mat4.lookAt(
      \[0, 0, 1.5\],  // position
      \[0, 0, 0\],    // target
      \[0, 1, 0\],    // up
    );
    const viewProjection = mat4.multiply(projection, view);
    mat4.rotateY(viewProjection, time, matrixValue);
    mat4.rotateX(matrixValue, time \* 0.5, matrixValue);

    // Copy the uniform values to the GPU
    device.queue.writeBuffer(uniformBuffer, 0, uniformValues);

    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
    pass.setVertexBuffer(0, vertexBuffer);
    pass.setBindGroup(0, bindGroup);
    pass.draw(kNumPoints);
    pass.end();

    const commandBuffer = encoder.finish();
    device.queue.submit(\[commandBuffer\]);

    requestAnimationFrame(render);
  }

  requestAnimationFrame(render);

We also switched to a `requestAnimationFrame` loop.

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-3d-1px.html)

That‚Äôs hard to see, so, to apply the techniques above, we just add the in quad position just like we did previously.

struct Vertex {
  @location(0) position: vec4f,
};

struct Uniforms {
  matrix: mat4x4f,
+  resolution: vec2f,
+  size: f32,
};

struct VSOutput {
  @builtin(position) position: vec4f,
};

@group(0) @binding(0) var<uniform> uni: Uniforms;

@vertex fn vs(
    vert: Vertex,
+    @builtin(vertex\_index) vNdx: u32,
) -> VSOutput {
+  let points = array(
+    vec2f(-1, -1),
+    vec2f( 1, -1),
+    vec2f(-1,  1),
+    vec2f(-1,  1),
+    vec2f( 1, -1),
+    vec2f( 1,  1),
+  );
  var vsOut: VSOutput;
+  let pos = points\[vNdx\];
  let clipPos = uni.matrix \* vert.position;
+  let pointPos = vec4f(pos \* uni.size / uni.resolution, 0, 0);
-  vsOut.position = clipPos;
+  vsOut.position = clipPos + pointPos;
  return vsOut;
}

@fragment fn fs(vsOut: VSOutput) -> @location(0) vec4f {
  return vec4f(1, 0.5, 0.2, 1);
}

Unlike the previous example we won‚Äôt use a different size for each vertex. Instead we‚Äôll pass a single size for all vertices.

\-  const uniformValues = new Float32Array(16);
+  const uniformValues = new Float32Array(16 + 2 + 1 + 1);
  const uniformBuffer = device.createBuffer({
    size: uniformValues.byteLength,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY\_DST,
  });
  const kMatrixOffset = 0;
+  const kResolutionOffset = 16;
+  const kSizeOffset = 18;
  const matrixValue = uniformValues.subarray(
      kMatrixOffset, kMatrixOffset + 16);
+  const resolutionValue = uniformValues.subarray(
+      kResolutionOffset, kResolutionOffset + 2);
+  const sizeValue = uniformValues.subarray(
+      kSizeOffset, kSizeOffset + 1);

We need to set the resolution as we did above, and we need to set a size

  function render(time) {
    ...
+    // Set the size in the uniform buffer
+    sizeValue\[0\] = 10;

    const fov = 90 \* Math.PI / 180;
    const aspect = canvas.clientWidth / canvas.clientHeight;
    const projection = mat4.perspective(fov, aspect, 0.1, 50);
    const view = mat4.lookAt(
      \[0, 0, 1.5\],  // position
      \[0, 0, 0\],    // target
      \[0, 1, 0\],    // up
    );
    const viewProjection = mat4.multiply(projection, view);
    mat4.rotateY(viewProjection, time, matrixValue);
    mat4.rotateX(matrixValue, time \* 0.5, matrixValue);

+    // Update the resolution in the uniform buffer
+    resolutionValue.set(\[canvasTexture.width, canvasTexture.height\]);

    // Copy the uniform values to the GPU
    device.queue.writeBuffer(uniformBuffer, 0, uniformValues);

And, like we did before, we need to switch from drawing points to drawing instanced quads

  const pipeline = device.createRenderPipeline({
    label: '3d points',
    layout: 'auto',
    vertex: {
      module,
      buffers: \[
        {
          arrayStride: (3) \* 4, // 3 floats, 4 bytes each
+          stepMode: 'instance',
          attributes: \[
            {shaderLocation: 0, offset: 0, format: 'float32x3'},  // position
          \],
        },
      \],
    },
    fragment: {
      module,
      targets: \[
        {
         format: presentationFormat,
        },
      \],
    },
-    primitive: {
-      topology: 'point-list',
-    },
  });

  ...

  function render(time) {

    ...

-    pass.draw(kNumPoints);
+    pass.draw(6, kNumPoints);

    ...

This gives us points in 3D. They even scale based on their distance from the camera.

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-3d.html)

## Fixed size 3d points

What if we want the points to stay a fixed size?

Recall from [the article on perspective projection](webgpu-perspective-projection.html) that the GPU divides the position we return from the vertex shader by W. This divide gives us perspective by making things further way appear smaller. So, for points we don‚Äôt want to change size we just need to multiply them by that W so after they‚Äôre divided they‚Äôll be the value we really wanted.

    var vsOut: VSOutput;
    let pos = points\[vNdx\];
    let clipPos = uni.matrix \* vert.position;
-    let pointPos = vec4f(pos \* uni.size / uni.resolution, 0, 0);
+    let pointPos = vec4f(pos \* uni.size / uni.resolution \* clipPos.w, 0, 0);
    vsOut.position = clipPos + pointPos;
    return vsOut;

And now they stay the same size

[click here to open in a separate window](/webgpu/lessons/../webgpu-points-3d-fixed-size.html)

### Why doesn't WebGPU support points larger than 1x1 pixel?

WebGPU is based on native GPU APIs like Vulkan, Metal, DirectX, and even OpenGL. Unfortunately, those APIs do not agree with each other on what it means to support rendering points. Some APIs have device dependent limits on the size of points. Some APIs don't render a point if its center is outside of clip space while others do. In some APIs, this second issue is up to the driver. All of that means WebGPU decided to do the portable thing and only support 1x1 sized pixels.

The good thing is it's easy to support larger points yourself as shown above. The solutions above are portable across devices, they have no limit on the size of a point and they consistently clip points across devices. They draw the portion of any point that is inside clip space regardless of if the point's center is outside of clip space.

Even better, these solutions are more flexible. For example rotating points is not a thing supported by native APIs. By implementing our own solutions we can easily add more features making things even more flexible.

English Spanish Êó•Êú¨Ë™û ÌïúÍµ≠Ïñ¥ –†—É—Å—Å–∫–∏–π T√ºrk√ße –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ ÁÆÄ‰Ωì‰∏≠Êñá

*   Basics

*   [Fundamentals](/webgpu/lessons/webgpu-fundamentals.html)
*   [Inter-stage Variables](/webgpu/lessons/webgpu-inter-stage-variables.html)
*   [Uniforms](/webgpu/lessons/webgpu-uniforms.html)
*   [Storage Buffers](/webgpu/lessons/webgpu-storage-buffers.html)
*   [Vertex Buffers](/webgpu/lessons/webgpu-vertex-buffers.html)
*   Textures

*   [Textures](/webgpu/lessons/webgpu-textures.html)
*   [Loading Images](/webgpu/lessons/webgpu-importing-textures.html)
*   [Using Video](/webgpu/lessons/webgpu-textures-external-video.html)
*   [Cube Maps](/webgpu/lessons/webgpu-cube-maps.html)
*   [Storage Textures](/webgpu/lessons/webgpu-storage-textures.html)
*   [Multisampling / MSAA](/webgpu/lessons/webgpu-multisampling.html)

*   [Constants](/webgpu/lessons/webgpu-constants.html)
*   [Data Memory Layout](/webgpu/lessons/webgpu-memory-layout.html)
*   [Transparency and Blending](/webgpu/lessons/webgpu-transparency.html)
*   [Bind Group Layouts](/webgpu/lessons/webgpu-bind-group-layouts.html)
*   [Copying Data](/webgpu/lessons/webgpu-copying-data.html)
*   [Optional Features and Limits](/webgpu/lessons/webgpu-limits-and-features.html)
*   [Timing Performance](/webgpu/lessons/webgpu-timing.html)
*   [WGSL](/webgpu/lessons/webgpu-wgsl.html)
*   [How It Works](/webgpu/lessons/webgpu-how-it-works.html)
*   [Compatibility Mode](/webgpu/lessons/webgpu-compatibility-mode.html)

*   3D Math

*   [Translation](/webgpu/lessons/webgpu-translation.html)
*   [Rotation](/webgpu/lessons/webgpu-rotation.html)
*   [Scale](/webgpu/lessons/webgpu-scale.html)
*   [Matrix Math](/webgpu/lessons/webgpu-matrix-math.html)
*   [Orthographic Projection](/webgpu/lessons/webgpu-orthographic-projection.html)
*   [Perspective Projection](/webgpu/lessons/webgpu-perspective-projection.html)
*   [Cameras](/webgpu/lessons/webgpu-cameras.html)
*   [Matrix Stacks](/webgpu/lessons/webgpu-matrix-stacks.html)
*   [Scene Graphs](/webgpu/lessons/webgpu-scene-graphs.html)

*   Lighting

*   [Directional Lighting](/webgpu/lessons/webgpu-lighting-directional.html)
*   [Point Lighting](/webgpu/lessons/webgpu-lighting-point.html)
*   [Spot Lighting](/webgpu/lessons/webgpu-lighting-spot.html)

*   Techniques

*   2D

*   [Large Clip Space Triangle](/webgpu/lessons/webgpu-large-triangle-to-cover-clip-space.html)

*   3D

*   [Environment maps](/webgpu/lessons/webgpu-environment-maps.html)
*   [Skyboxes](/webgpu/lessons/webgpu-skybox.html)

*   Post Processing

*   [Basic CRT Effect](/webgpu/lessons/webgpu-post-processing.html)

*   Compute Shaders

*   [Compute Shader Basics](/webgpu/lessons/webgpu-compute-shaders.html)
*   [Image Histogram](/webgpu/lessons/webgpu-compute-shaders-histogram.html)
*   [Image Histogram Part 2](/webgpu/lessons/webgpu-compute-shaders-histogram-part-2.html)

*   Misc

*   [Resizing the Canvas](/webgpu/lessons/webgpu-resizing-the-canvas.html)
*   [Multiple Canvases](/webgpu/lessons/webgpu-multiple-canvases.html)
*   [Points](/webgpu/lessons/webgpu-points.html)
*   [WebGPU from WebGL](/webgpu/lessons/webgpu-from-webgl.html)
*   [Speed and Optimization](/webgpu/lessons/webgpu-optimization.html)
*   [Debugging and Errors](/webgpu/lessons/webgpu-debugging.html)
*   [Resources / References](/webgpu/lessons/webgpu-resources.html)
*   [WGSL Function Reference](/webgpu/lessons/webgpu-wgsl-function-reference.html)
*   [WGSL Offset Computer](/webgpu/lessons/resources/wgsl-offset-computer.html)

*   [github](https://github.com/webgpu/webgpufundamentals)
*   [Tour of WGSL](https://google.github.io/tour-of-wgsl/)
*   [WebGPU API Reference](https://gpuweb.github.io/types/)
*   [WebGPU Spec](https://www.w3.org/TR/webgpu/)
*   [WGSL Spec](https://www.w3.org/TR/WGSL/)
*   [WebGPUReport.org](https://webgpureport.org)
*   [Web3DSurvey.com](https://web3dsurvey.com/webgpu)

Questions? [Ask on stackoverflow](http://stackoverflow.com/questions/tagged/webgpu).

[Suggestion](https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&labels=suggested+topic&template=suggest-topic.md&title=%5BSUGGESTION%5D)? [Request](https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&labels=&template=request.md&title=)? [Issue](https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&labels=bug+%2F+issue&template=bug-issue-report.md&title=)? [Bug](https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&labels=bug+%2F+issue&template=bug-issue-report.md&title=)?

var disqus\_config = function () { this.page.url = \`${window.location.origin}${window.location.pathname}\`; this.page.identifier = \`WebGPU Points\`; }; (function() { // DON'T EDIT BELOW THIS LINE if (window.location.hostname.indexOf("webgpufundamentals.org") < 0) { return; } var d = document, s = d.createElement('script'); s.src = 'https://webgpufundamentals-org.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })();

Please enable JavaScript to view the [comments powered by Disqus.](http://disqus.com/?ref_noscript)

[comments powered by Disqus](http://disqus.com)

const settings = { contribTemplate: "Thank you <a href=\\"${html\_url}\\"><img src=\\"${avatar\_url}\\"> ${login}</a><br>for <a href=\\"https://github.com/${owner}/${repo}/commits?author=${login}\\">${contributions} contributions</a>", owner: "gfxfundamentals", repo: "webgpufundamentals", }; if (typeof module === 'object') {window.module = module; module = undefined;} window.dataLayer = window.dataLayer || \[\]; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-92BFT5PE4H');